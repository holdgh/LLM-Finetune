马尔可夫决策过程（Markov Decision Process, MDP）难以有效捕捉全局信息的根本原因在于其核心假设——**马尔可夫性（Markov Property）**。这一特性虽简化了问题建模，却天然限制了全局信息的整合能力。以下是具体分析：

---

### **一、马尔可夫性的本质限制**
#### **1. 状态转移的局部性**
- **定义**：  
  MDP要求下一状态仅依赖当前状态和动作，与历史无关：  
  $$P(s_{t+1} \mid s_t, a_t, s_{t-1}, a_{t-1}, \cdots) = P(s_{t+1} \mid s_t, a_t)$$  
- **问题**：  
  状态转移函数无法建模长程依赖（如围棋中第10步的落子影响最终胜负）。

#### **2. 奖励函数的近视性**
- **定义**：  
  即时奖励仅取决于当前状态和动作：  
  $$r_t = R(s_t, a_t)$$  
- **问题**：  
  无法直接表达延迟奖励（Delayed Reward）。例如，在投资决策中，早期研发投入的回报可能在数年后才显现。

---

### **二、全局信息缺失的三大表现**
#### **1. 历史遗忘（History Amnesia）**
- **案例**：  
  在导航任务中，若智能体经过同一路口两次，MDP无法识别“绕圈”行为（需记忆历史路径）。
- **数学体现**：  
  状态空间不包含历史轨迹信息 $s_t \neq f(s_0, a_0, \cdots, s_{t-1}, a_{t-1})$。

#### **2. 长期因果断裂**
- **案例**：  
  在供应链优化中，某次原材料采购（动作 $a_t$）可能因三个月后的市场价格波动（状态 $s_{t+k}$）导致亏损，但MDP无法建立跨时段的因果链。
- **数学体现**：  
  价值函数 $V^\pi(s)$ 仅依赖当前状态，而非全局策略一致性。

#### **3. 环境动态的局部视角**
- **案例**：  
  在多智能体博弈（如《星际争霸》）中，对手的全局战略布局无法通过单步状态观测获得。
- **数学体现**：  
  转移概率 $P(s' \mid s,a)$ 不建模其他智能体的长期意图。

---

### **三、MDP的扩展模型与改进方向**
为缓解全局信息缺失，研究者提出以下扩展方案：

#### **1. 部分可观马尔可夫决策过程（POMDP）**
- **机制**：  
  引入隐含状态（Belief State），通过观测序列推断全局信息。  
- **局限性**：  
  信念状态计算复杂度指数级增长（Curse of Dimensionality）。

#### **2. 状态空间增强**
- **方法**：  
  人工设计状态包含历史信息（如将过去 $k$ 步状态拼接为 $s_t = [s_{t-k}, \cdots, s_t]$）。  
- **案例**：  
  LSTM在DRL中编码历史观测，解决部分观测问题。

#### **3. 奖励塑形（Reward Shaping）**
- **机制**：  
  设计奖励函数 $r'(s,a,s')$ 隐式传递全局目标信息。  
- **风险**：  
  不合理的塑形可能导致目标偏移（Goal Misalignment）。

#### **4. 分层强化学习（HRL）**
- **机制**：  
  高层策略制定长期目标，底层策略执行短期动作。  
- **案例**：  
  Option Framework 中的宏动作（Macro-Action）跨越多个时间步。

---

### **四、全局信息捕捉的替代方案**
#### **1. 基于模型的强化学习（MBRL）**
- **方法**：  
  学习环境动力学模型 $f(s_{t+1} \mid s_t, a_t)$，通过树搜索（如MCTS）规划全局路径。  
- **优势**：  
  AlphaGo 通过MCTS评估棋局长期价值。

#### **2. 注意力机制与Transformer**
- **方法**：  
  用自注意力层显式建模状态间的长程依赖。  
- **案例**：  
  Decision Transformer 将强化学习转化为序列预测问题。

#### **3. 图神经网络（GNN）**
- **适用场景**：  
  状态可表示为图结构（如社交网络、交通系统）。  
- **优势**：  
  通过消息传递聚合全局拓扑信息。

---

### **五、MDP适用场景 vs 全局信息需求场景**
| **场景类型**         | 适用MDP程度 | 案例                     | 需补充技术               |
|----------------------|-------------|--------------------------|--------------------------|
| **局部决策**         | ★★★★★       | 机器人避障              | 无需                     |
| **长程依赖**         | ★☆☆☆☆        | 药物研发（10年周期）    | HRL + MBRL               |
| **多智能体协作**     | ★★☆☆☆        | 自动驾驶车队协同        | GNN + 集中式训练         |
| **部分可观测环境**   | ★☆☆☆☆        | 扑克游戏（隐藏手牌）    | POMDP + LSTM             |

---

### **总结**
马尔可夫决策过程因**状态转移的局部性**和**奖励函数的近视性**，难以直接捕捉全局信息。其根本约束源于马尔可夫性假设，该假设虽带来计算可行性，却牺牲了长期依赖建模能力。突破方向包括：
1. **扩展状态空间**（如LSTM、注意力机制）  
2. **分层策略分解**（HRL将全局目标拆解为局部子任务）  
3. **环境模型预测**（MBRL通过仿真预见长期影响）  

在实际应用中，需根据任务复杂度权衡：简单控制任务（如机械臂抓取）可沿用MDP；复杂全局决策（如企业战略规划）需融合外部记忆、图网络等超马尔可夫技术。